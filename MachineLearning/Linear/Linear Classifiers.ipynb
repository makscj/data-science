{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Why linear functions?\n",
    "\n",
    "Suppose we are looking at the space of Boolean functions over 4 bits. There are 16 possible outputs for these functions, and each output has 2 ways it can be filled. \n",
    "That means there are 2^16 possible functions. Suppose we only see 7 outputs, we cannot know what the rest are without seeing them. And think of an adversary filling\n",
    "in the labels every time you make a guess at the function. \n",
    "\n",
    "Now imagine we are in 2 dimensional Real space, R^2, there are infinite possibilities for the classifing function. \n",
    "\n",
    "### How could we possibly learn anything? \n",
    "\n",
    "The solution is to **restrict the search space**. A _hypothesis space_  is the set of all possible functions we can consider. \n",
    "\n",
    "In the above example our space is the space of all 4-bit boolean functions. Instead we choose a hypothesis space that is smaller than the space of all functions. For example:\n",
    "\n",
    "- Only simple conjunctions (with 4 variables, there are 16 conjunctions without negation)\n",
    "- Simple disjunctions\n",
    "- m of n rules: Fix a set of n variables, at least m of them must be true. \n",
    "- Linear functions\n",
    "\n",
    "## Regression vs Classification \n",
    "\n",
    "Linear regression is about predicting real valued outputs, linear classifications is about predicting a discrete class label. \n",
    "\n",
    "## Linear Classifiers\n",
    "\n",
    "Inputs are $d$ dimensional vectors denoted by $\\mathbf{x}$, where the output is a label $y \\in \\{-1,1\\}$.\n",
    "\n",
    "_Linear Threshold Units_ classify an example $\\mathbf{x}$ using parameters $\\mathbf{w}$ (a $d$ dimensional vector) and $b$ (a real number) according to the following classification rule\n",
    "\n",
    "$$\\text{Output} = \\text{sign}(\\mathbf{w}^T\\mathbf{x} + b) = \\text{sign}\\bigg(\\sum_iw_ix_i + b\\bigg)$$\n",
    "\n",
    "$$ \\mathbf{w}^T\\mathbf{x} + b \\geq 0 \\rightarrow y = + 1$$\n",
    "$$ \\mathbf{w}^T\\mathbf{x} + b \\lt 0 \\rightarrow y = - 1$$\n",
    "\n",
    "Note: $b$ is called the _bias_ term. \n",
    "\n",
    "In higher dimensions, a linear classifier represents a hyperplane that separates the space into two half-spaces. \n",
    "\n",
    "To save space on notation, we can stop writing $b$ at each step and rewrite\n",
    "\n",
    "$$ \\mathbf{x} \\text{ as } \\begin{bmatrix} \\mathbf{x} \\\\ 1 \\end{bmatrix}\n",
    "\\text{ and } \\mathbf{w} \\text{ as } \\begin{bmatrix} \\mathbf{w} \\\\ b \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Therefore the product is the same. This is equivalent to adding a feature that is a constant, always 1. This also increases the dimensionality of the space, with the resulting prediction vector $\\mathbf{w}$ goes through the origin. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expressiveness\n",
    "\n",
    "What functions do **linear classifiers** represent? \n",
    "\n",
    "Many Boolean functions are _linearly separable_ (not all though). In comparison, decision trees can represent any Boolean function. \n",
    "\n",
    "### Conjunctions and Disjunctions\n",
    "\n",
    "We can write $y = x_1 \\land x_2 \\land x_3$ as the linear expression $y = 1$ if $x_1 + x_2 + x_3 \\geq 3$. We can also express negations, using $(1-x)$ in the linear threshold if $x$ is negated. \n",
    "\n",
    "### m-of-n functions\n",
    "\n",
    "Rules:\n",
    "- There is a fixed set of $n$ variables\n",
    "- $y$ is True if, and only if, at least $m$ of them are True\n",
    "- All other variables are ignored. \n",
    "\n",
    "Suppose we have five Boolean variables: $x_1, x_2, x_3, x_4, x_5$\n",
    "\n",
    "What is a linear threshold unit that is equivalent to the classification rule: at least 2 of $\\{x_1, x_2, x_3\\}$?\n",
    "\n",
    "$x_1 + x_2 + x_3 \\geq 2$\n",
    "\n",
    "### Not all functions are linearly separable\n",
    "\n",
    "The parity function, XOR, is not separable. We cannot draw a line that separates the two classes. \n",
    "\n",
    "Many non-trivial boolean functions, such as $y = (x_1 \\land x_2) \\lor (x_3 \\land \\lnot x_4)$. This function is not linear in the four variables\n",
    "\n",
    "But even these functions can be _made_ linear, we just need to change the representation. \n",
    "\n",
    "We can do this by blowing up the feature space, for example by using a feature conjunction: $(x,x^2)$.\n",
    "\n",
    "### Noise\n",
    "\n",
    "What about noise? How much noise do we allow for? \n",
    "\n",
    "### Bias Term\n",
    "\n",
    "Why is the bias term $b$ needed? If $b$ is zero, then we are restricting the learner only to hyperplanes that go through the origin, which may not be expressive enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Mean Squares Regression\n",
    "\n",
    "Suppose we want to predict the mileage of a car from its weight and age. We want a function that can predict mileage using $x_1$ (weight) and $x_2$ (age). The assumption is a linear function of the inputs: $\\text{Mileage} = w_0 + w_1x_1 + w_2x_2$. We want to learn using the training data to find the best possible value of $\\mathbf{w}$. For prediction, given values $x_1,x_2$ for a new car, use the learned $\\mathbf{w}$ to predict the Mileage for the new car. \n",
    "\n",
    "The parameters of the model, $w_1, w_2,w_3$ are also known as the weights, and collectively form a vector $\\mathbf{w}$. \n",
    "\n",
    "Therefore, the strategy for _linear regression_ is we have:\n",
    "- Inputs are vectors: $\\mathbf{x} \\in \\mathbb{R}^d$\n",
    "- Outputs are real numbers: $y \\in \\mathbb{R}$\n",
    "- We have training set $D = \\{(\\mathbf{x}_1,y_1),(\\mathbf{x}_2,y_2),\\dots\\}$\n",
    "- We want to approximate $y$ as $y = \\mathbf{w}^T\\mathbf{x}$ where $\\mathbf{w}$ is the learned weight vector in $\\mathbb{R}^d$.\n",
    "\n",
    "How do we know which weight vector is the best one for a training set? For an input $(\\mathbf{x}_i,y_i)$ in the training set, the _cost_ of a mistake is \n",
    "$$\\left| y_i - \\mathbf{w}^T\\mathbf{x} \\right|$$\n",
    "So we can define the cost (or loss) for a particular weight vector $\\mathbf{w}$ to be\n",
    "\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^{m}(y_i - \\mathbf{w}^T\\mathbf{x_i})^2$$\n",
    "\n",
    "This is the sum of squared costs over the training set. One strategy for learning: find the $\\mathbf{w}$ with the least cost on this data.\n",
    "\n",
    "Learning: minimizing the mean squared error; Least Mean Squares (LMS) regression. \n",
    "\n",
    "$$ \\min_{\\mathbf{w}} \\frac{1}{2} \\sum_{i=1}^{m}\\big(y_i - \\mathbf{w}^T\\mathbf{x_i}\\big)^2$$\n",
    "\n",
    "Then we can use one of the different strategies that exist for _learning by optimization_. __Gradient Descent__ is a popular algorithm. (For this particular minimization objective, there is an analytical solution, so no need for gradient descent)\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "The intuition is that the gradient is the direction of the steepest increase in the function. To get to the minimum, go to the opposite direction. \n",
    "\n",
    "The general strategy for minimizing a function $J(\\mathbf{w})$.\n",
    "\n",
    "* Start with an initial guess for $\\mathbf{w}$, say $\\mathbf{w}^0$\n",
    "* Iterate until convergence:\n",
    "    * Compute the gradient of the gradient of $J$ at $\\mathbf{w}^t$\n",
    "    * Update $\\mathbf{w}^t$ to get $\\mathbf{w}^{t+1} by taking a step in the opposite direction of the gradient. \n",
    "    \n",
    "Giving us the algorithm:\n",
    "\n",
    "1. Initialize $\\mathbf{w}^0$\n",
    "2. For $t=0,1,2,\\dots$ (until total error is below a threshold)\n",
    "    1. Compute the gradient of $J(\\mathbf{w})$ at $\\mathbf{w}^t$. Call it $\\nabla J(\\mathbf{w}^t)$\n",
    "    2. Update $\\mathbf{w}$ as follows:\n",
    "    $\\mathbf{w}^{t+1} = \\mathbf{w}^t - r\\nabla J(\\mathbf{w}^t) $\n",
    "    3. $r$ is known as the learning rate, for now it's just a small constant. \n",
    "    \n",
    "    \n",
    "What is the gradient of $J(\\mathbf{w})$? Remember $\\mathbf{w}$ is a vector, so\n",
    "\n",
    "$$\\nabla J(\\mathbf{w}^t) = \\Bigg[\\frac{\\partial J}{\\partial w_1},\\frac{\\partial J}{\\partial w_2},\\dots,\\frac{\\partial J}{\\partial w_d}\\Bigg]$$\n",
    "\n",
    "and looking at $\\frac{\\partial J}{\\partial w_j}$:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w_j} = - \\sum_{i=1}^m \\big(y_i - \\mathbf{w}^T\\mathbf{x}_i\\big)x_{ij}$$\n",
    "Which if you notice is just the sum of error $\\times$ input.\n",
    "\n",
    "This algorithm is guaranteed to converge to the minimum of $J$ if $r$ is small enough, because the objective $J$ is a _convex_ function.\n",
    "\n",
    "Notice that we use a sum $\\sum$ in computing our partials, meaning the weight vector is not updated until _all_ errors are calculated. Why not make early updates to the weight vector as soon as we encounter errors instead of waiting for a full pass over the data? \n",
    "\n",
    "## Incremental/Stochastic Gradient Descent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "\n",
    "Error driven learning, where we update the hypothesis if there is an error. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
